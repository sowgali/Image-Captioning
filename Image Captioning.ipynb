{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '/annotations/'\n",
    "\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file('captions.zip', cache_subdir=os.path.abspath('.'),\n",
    "                                            origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip', \n",
    "                                            extract = True)\n",
    "    annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "else:\n",
    "    annotation_file = os.path.abspath('.') + '/annotations/captions_train2014.json'\n",
    "\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "    image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "    PATH = os.path.dirname(image_zip) + image_folder\n",
    "    os.remove(image_zip)\n",
    "else:\n",
    "    PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "num_examples = 30000\n",
    "train_captions = train_captions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 414113)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions), len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5000\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = top_k, oov_token = \"<unk>\", filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector, cap_vector, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train)//BATCH_SIZE\n",
    "features_shape = 2048\n",
    "attention_feature_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc1(output)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        x = self.fc2(x)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 0.8529\n",
      "Epoch 2 Batch 100 Loss 0.8514\n",
      "Epoch 2 Batch 200 Loss 0.8255\n",
      "Epoch 2 Batch 300 Loss 0.7713\n",
      "Epoch 2 Loss 0.787965\n",
      "Time taken for 1 epoch 1695.2567048072815 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7841\n",
      "Epoch 3 Batch 100 Loss 0.7716\n",
      "Epoch 3 Batch 200 Loss 0.7014\n",
      "Epoch 3 Batch 300 Loss 0.7022\n",
      "Epoch 3 Loss 0.718155\n",
      "Time taken for 1 epoch 1416.8525476455688 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.7401\n",
      "Epoch 4 Batch 100 Loss 0.6163\n",
      "Epoch 4 Batch 200 Loss 0.6575\n",
      "Epoch 4 Batch 300 Loss 0.6230\n",
      "Epoch 4 Loss 0.674285\n",
      "Time taken for 1 epoch 1456.4912602901459 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6640\n",
      "Epoch 5 Batch 100 Loss 0.6139\n",
      "Epoch 5 Batch 200 Loss 0.6799\n",
      "Epoch 5 Batch 300 Loss 0.6523\n",
      "Epoch 5 Loss 0.640055\n",
      "Time taken for 1 epoch 1480.2071058750153 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.6487\n",
      "Epoch 6 Batch 100 Loss 0.5871\n",
      "Epoch 6 Batch 200 Loss 0.6793\n",
      "Epoch 6 Batch 300 Loss 0.6081\n",
      "Epoch 6 Loss 0.607235\n",
      "Time taken for 1 epoch 1525.0901188850403 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.5608\n",
      "Epoch 7 Batch 100 Loss 0.6375\n",
      "Epoch 7 Batch 200 Loss 0.5863\n",
      "Epoch 7 Batch 300 Loss 0.5385\n",
      "Epoch 7 Loss 0.577284\n",
      "Time taken for 1 epoch 1561.563690662384 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.5661\n",
      "Epoch 8 Batch 100 Loss 0.5856\n",
      "Epoch 8 Batch 200 Loss 0.5626\n",
      "Epoch 8 Batch 300 Loss 0.5499\n",
      "Epoch 8 Loss 0.548474\n",
      "Time taken for 1 epoch 1541.3001618385315 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5719\n",
      "Epoch 9 Batch 100 Loss 0.5195\n",
      "Epoch 9 Batch 200 Loss 0.5429\n",
      "Epoch 9 Batch 300 Loss 0.4927\n",
      "Epoch 9 Loss 0.520409\n",
      "Time taken for 1 epoch 1463.660698890686 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4609\n",
      "Epoch 10 Batch 100 Loss 0.5175\n",
      "Epoch 10 Batch 200 Loss 0.4964\n",
      "Epoch 10 Batch 300 Loss 0.4740\n",
      "Epoch 10 Loss 0.492385\n",
      "Time taken for 1 epoch 1276.34929728508 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.4844\n",
      "Epoch 11 Batch 100 Loss 0.5307\n",
      "Epoch 11 Batch 200 Loss 0.4235\n",
      "Epoch 11 Batch 300 Loss 0.4429\n",
      "Epoch 11 Loss 0.465108\n",
      "Time taken for 1 epoch 1276.7352747917175 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4861\n",
      "Epoch 12 Batch 100 Loss 0.4325\n",
      "Epoch 12 Batch 200 Loss 0.4285\n",
      "Epoch 12 Batch 300 Loss 0.4303\n",
      "Epoch 12 Loss 0.439482\n",
      "Time taken for 1 epoch 1288.7704229354858 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.4105\n",
      "Epoch 13 Batch 100 Loss 0.4149\n",
      "Epoch 13 Batch 200 Loss 0.4084\n",
      "Epoch 13 Batch 300 Loss 0.4315\n",
      "Epoch 13 Loss 0.414500\n",
      "Time taken for 1 epoch 1300.960771560669 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.4429\n",
      "Epoch 14 Batch 100 Loss 0.3752\n",
      "Epoch 14 Batch 200 Loss 0.4048\n",
      "Epoch 14 Batch 300 Loss 0.3912\n",
      "Epoch 14 Loss 0.393309\n",
      "Time taken for 1 epoch 1289.4843080043793 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.4185\n",
      "Epoch 15 Batch 100 Loss 0.3603\n",
      "Epoch 15 Batch 200 Loss 0.3783\n",
      "Epoch 15 Batch 300 Loss 0.3592\n",
      "Epoch 15 Loss 0.378800\n",
      "Time taken for 1 epoch 1283.0434272289276 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.3611\n",
      "Epoch 16 Batch 100 Loss 0.3271\n",
      "Epoch 16 Batch 200 Loss 0.3322\n",
      "Epoch 16 Batch 300 Loss 0.4007\n",
      "Epoch 16 Loss 0.346385\n",
      "Time taken for 1 epoch 1289.7327427864075 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.3419\n",
      "Epoch 17 Batch 100 Loss 0.3421\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
